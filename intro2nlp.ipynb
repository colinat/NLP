{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to NLP\n",
    "---\n",
    "\n",
    "Data comes in many different forms: time stamps, sensor readings, images, categorical labels, and so much more. But text is still some of the most valuable data out there for those who know how to use it.\n",
    "\n",
    "### NLP with spaCy\n",
    "\n",
    "In this note, I'll be using the NLP library (spaCy) to take on some of the most important tasks in working with text. spaCy relies on models that are language-specific and come in different sizes. You can load a spaCy model with `spacy.load`.\n",
    "\n",
    "For example, here's how you would load the English language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note***: *Before you can load spacy's \"en_core_web_sm\", you need to first download the library file by running the following command in the command prompt:*\n",
    "```python\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "*Alternatively, you can run \"!python -m spacy download en_core_web_sm\" in Jupyter notebook.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing\n",
    "\n",
    "With the model loaded, you can process text like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tea\n",
      "is\n",
      "healthy\n",
      "and\n",
      "calming\n",
      ",\n",
      "do\n",
      "n't\n",
      "you\n",
      "think\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Tea is healthy and calming, don't you think?\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nlp('some_text')` returns a document object that contains tokens. A token is a unit of text in the document, such as individual words and punctuation. SpaCy splits contractions like \"don't\" into two tokens, \"do\" and \"n't\". You can see the tokens by iterating through the document.\n",
    "\n",
    "Iterating through a document gives you token objects. Each of these tokens comes with additional information. In most cases, the important ones are `token.lemma_` and `token.is_stop`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "\n",
    "There are a few types of preprocessing to improve how we model with words. The first is \"lemmatizing.\" The \"lemma\" of a word is its base form. For example, \"walk\" is the lemma of the word \"walking\". So, when you lemmatize the word walking, you would convert it to walk.\n",
    "\n",
    "It's also common to remove stopwords. Stopwords are words that occur frequently in the language and don't contain much information. English stopwords include \"the\", \"is\", \"and\", \"but\", \"not\".\n",
    "\n",
    "`token.lemma_` returns the token's lemma, while `token.is_stop` returns a boolean True if the token is a stopword (and False otherwise). For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token \t\tLemma \t\tStopword\n",
      "----------------------------------------\n",
      "Tea\t\ttea\t\tFalse\n",
      "is\t\tbe\t\tTrue\n",
      "healthy\t\thealthy\t\tFalse\n",
      "and\t\tand\t\tTrue\n",
      "calming\t\tcalming\t\tFalse\n",
      ",\t\t,\t\tFalse\n",
      "do\t\tdo\t\tTrue\n",
      "n't\t\tn't\t\tTrue\n",
      "you\t\tyou\t\tTrue\n",
      "think\t\tthink\t\tFalse\n",
      "?\t\t?\t\tFalse\n"
     ]
    }
   ],
   "source": [
    "print(f\"Token \\t\\tLemma \\t\\tStopword\".format('Token', 'Lemma', 'Stopword'))\n",
    "print(\"-\"*40)\n",
    "for token in doc:\n",
    "    print(f\"{str(token)}\\t\\t{token.lemma_}\\t\\t{token.is_stop}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are lemmas and identifying stopwords important? Language data has a lot of noise mixed in with informative content. In the sentence above, the important words are tea, healthy and calming. Removing stop words might help the predictive model focus on relevant words. Lemmatizing similarly helps by combining multiple forms of the same word into one base form (\"calming\", \"calms\", \"calmed\" would all change to \"calm\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the techniques learnt on 'A Million News Headlines' [dataset](https://www.kaggle.com/therohk/million-headlines), which is a corpus of over one million news article headlines published by the ABC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20030219</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20030219</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20030219</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                                      headline_text\n",
       "0      20030219  aba decides against community broadcasting lic...\n",
       "1      20030219     act fire witnesses must be aware of defamation\n",
       "2      20030219     a g calls for infrastructure protection summit\n",
       "3      20030219           air nz staff in aust strike for pay rise\n",
       "4      20030219      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re  # For preprocessing\n",
    "import multiprocessing\n",
    "from time import time  # To time our operations\n",
    "\n",
    "text_data = pd.read_csv('../../datasets/news-headlines/abcnews-date-text.csv')\n",
    "text_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning:\n",
    "\n",
    "We are lemmatizing and removing the stopwords and non-alphabetic characters for each news headline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(doc):\n",
    "    # Lemmatizes and removes stopwords\n",
    "    # doc needs to be a spacy Doc object\n",
    "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
    "\n",
    "    # if a sentence is only one or two words long, the benefit for the training, e.g. Word2Vec, is very small, \n",
    "    # thus we discard short sentences of less than 3 words\n",
    "    if len(txt) > 2:\n",
    "        return ' '.join(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking advantage of spaCy `.pipe()` attribute to speed-up the cleaning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to clean up everything: 8.29 mins\n"
     ]
    }
   ],
   "source": [
    "# Yields generator object with non-alphabetic characters removed from text\n",
    "brief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in text_data['headline_text'])\n",
    "\n",
    "t = time()\n",
    "txt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000, \n",
    "                                         n_process=(multiprocessing.cpu_count()-1))]\n",
    "\n",
    "print('Time to clean up everything: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the results in a new column for easy comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20030219</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "      <td>aba decide community broadcasting licence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20030219</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "      <td>act fire witness aware defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20030219</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "      <td>g call infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "      <td>air nz staff aust strike pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "      <td>air nz strike affect australian traveller</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                                      headline_text  \\\n",
       "0      20030219  aba decides against community broadcasting lic...   \n",
       "1      20030219     act fire witnesses must be aware of defamation   \n",
       "2      20030219     a g calls for infrastructure protection summit   \n",
       "3      20030219           air nz staff in aust strike for pay rise   \n",
       "4      20030219      air nz strike to affect australian travellers   \n",
       "\n",
       "                                  clean_text  \n",
       "0  aba decide community broadcasting licence  \n",
       "1          act fire witness aware defamation  \n",
       "2    g call infrastructure protection summit  \n",
       "3          air nz staff aust strike pay rise  \n",
       "4  air nz strike affect australian traveller  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parse into dataframe \n",
    "text_data['clean_text'] = txt\n",
    "text_data.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
